import requests
from bs4 import BeautifulSoup
import os
import time
import json
import re
import urllib3
import traceback
import random
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from dotenv import load_dotenv

load_dotenv()

# ===[ì„¤ì • ì˜ì—­]==========================
# 30ë¶„ ì£¼ê¸°ë¡œ ë°”ê¿ˆ
CHECK_INTERVAL = 1800

# [NEW] - ì¬ì‹œë„í•˜ê¸°
MAX_RETRIES = 3
RETRY_DELAY = 60

DISCORD_WEBHOOK_URL = os.environ.get("cse_WEBHOOK_URL")
MONITOR_WEBHOOK_URL = os.environ.get("MONITOR_WEBHOOK_URL")

# ë°ì´í„° íŒŒì¼ ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)
DATA_FILE = os.path.join(DATA_DIR, "cse_data.json")

# ê²Œì‹œíŒ ëª©ë¡
TARGET_BOARDS = [
    {"id": "bachelor", "name": "í•™ì‚¬ê³µì§€", "url": "https://computer.cnu.ac.kr/computer/notice/bachelor.do?articleLimit=30"},
    {"id": "general", "name": "êµë‚´ì¼ë°˜ì†Œì‹", "url": "https://computer.cnu.ac.kr/computer/notice/notice.do?articleLimit=30"},
    {"id": "job", "name": "êµì™¸í™œë™Â·ì¸í„´Â·ì·¨ì—…", "url": "https://computer.cnu.ac.kr/computer/notice/job.do?articleLimit=30"},
    {"id": "project", "name": "ì‚¬ì—…ë‹¨ì†Œì‹", "url": "https://computer.cnu.ac.kr/computer/notice/project.do?articleLimit=30"}
]

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Connection': 'keep-alive'
}
# ==========================================

# ===[ì„¸ì…˜ ìƒì„±ê¸°]===
def get_session():
    """Network Level ì¬ì‹œë„ ì„¸ì…˜"""
    session = requests.Session()
    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

# ===[ID ì¶”ì¶œê¸°]===
def extract_article_id(link):
    match = re.search(r'articleNo=(\d+)', link)
    if match: return int(match.group(1))
    return 0

# ===[ë””ì½” ì „ì†¡ê¸°]===
def send_discord_batch_alert(category_name, new_notices):
    if not new_notices or not DISCORD_WEBHOOK_URL: return

    count = len(new_notices)
    message_content = f"### ğŸ“¢ [{category_name}] ìƒˆ ê¸€ {count}ê±´\n\n"

    for notice in new_notices:
        icon = "â–¶" if notice['is_top'] else "â–·"
        message_content += f"{icon} [{notice['title']}](<{notice['link']}>)\n"

    try:
        requests.post(DISCORD_WEBHOOK_URL, json={"content": message_content}, timeout=5)
        print(f"âœ‰ [ì „ì†¡ ì™„ë£Œ] {category_name} - {count}ê±´")
    except Exception as e:
        print(f"âš  [ì „ì†¡ ì‹¤íŒ¨] {e}")

# ===[ê´€ë¦¬ì ì•Œë¦¼ í•¨ìˆ˜]===
def send_simple_error_log(error_msg=None, is_fatal=False):
    """
    is_fatal=True ì¼ ë•Œë§Œ ê´€ë¦¬ì í˜¸ì¶œ
    """
    if not MONITOR_WEBHOOK_URL: return

    now = time.strftime('%Y-%m-%d %H:%M:%S')
    title = "ğŸš¨ **[CSE ë´‡ ì¹˜ëª…ì  ì˜¤ë¥˜]**" if is_fatal else "âš  **[CSE ë´‡ ê²½ê³ ]**"
    
    content = f"{title}\nì‹œê°„: {now}\n"
    if error_msg: content += f"ì—ëŸ¬: ```{error_msg}```"
    if is_fatal: content += "\n> ğŸ“¢ **ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. ë´‡ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.**"

    try: requests.post(MONITOR_WEBHOOK_URL, json={"content": content}, timeout=5)
    except: pass

# ===[ê²Œì‹œíŒ ê²€ì‚¬]===
def check_board(session, board_info, saved_data):
    """
    ì„±ê³µ ì‹œ: True/False ë°˜í™˜ (ë³€ê²½ì‚¬í•­ ìœ ë¬´)
    ì‹¤íŒ¨ ì‹œ: Exception ë°œìƒ (ìƒìœ„ ë£¨í”„ì—ì„œ ì¬ì‹œë„)
    """
    board_id = board_info["id"]
    board_name = board_info["name"]
    url = board_info["url"]

    print(f"â— [{board_name}] ë¶„ì„ ì¤‘...")

    # [enw] ì°¨ë‹¨ ë°©ì§€ ~ ì¬ì‹œë„ í• ë•Œë„ ì ìš©ë¨
    time.sleep(random.uniform(5, 10))

    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    response = session.get(url, headers=HEADERS, verify=False, timeout=30)
    response.encoding = 'utf-8'

    soup = BeautifulSoup(response.text, 'html.parser')
    rows = soup.select('table.board-table tbody tr')

    if not rows:
        # ì¬ì‹œë„ ë˜ì§€ê¸°
        raise Exception(f"ê²Œì‹œê¸€(tr) ì—†ìŒ - HTML êµ¬ì¡° ë³€ê²½ ë˜ëŠ” ì°¨ë‹¨ ì˜ì‹¬")

    last_id = saved_data.get(board_id, 0)
    new_notices = []
    max_id = last_id

    for row in rows:
        title_div = row.select_one('.b-title-box > a')
        if not title_div: continue

        title = title_div.get('title') or title_div.text.strip()
        title = title.replace("ìì„¸íˆ ë³´ê¸°", "").strip()

        href = title_div.get('href')
        if href.startswith('?'):
            base_url = url.split('?')[0]
            link = f"{base_url}{href}"
        else:
            link = href

        article_id = extract_article_id(link)
        if article_id == 0: continue

        row_classes = row.get('class', [])
        is_top = 'b-top-box' in row_classes

        if article_id > last_id:
            new_notices.append({
                "id": article_id, "title": title, "link": link, "is_top": is_top
            })
            if article_id > max_id: max_id = article_id

    # ìµœì´ˆ ì‹¤í–‰
    if last_id == 0 and max_id > 0:
        print(f"â˜ [{board_name}] ìµœì´ˆ ì‹¤í–‰ - ê¸°ì¤€ì (ID: {max_id})ë§Œ ì„¤ì •")
        saved_data[board_id] = max_id
        return True

    # ìƒˆ ê¸€ ì „ì†¡
    if new_notices:
        new_notices.sort(key=lambda x: x['id'])
        send_discord_batch_alert(board_name, new_notices)
        saved_data[board_id] = max_id
        return True

    return False

# ===[MAIN]===
def run_bot():
    print(f"ğŸš€ CSE ê³µì§€ë´‡ ì‹œì‘ (ì£¼ê¸°: {CHECK_INTERVAL}ì´ˆ, ì¬ì‹œë„: {MAX_RETRIES}íšŒ)")

    try:
        while True:
            print("\n" + "â”" * 40)
            print(f"â° ê²€ì‚¬ ì‹œì‘: {time.strftime('%Y-%m-%d %H:%M:%S')}")

            saved_data = {}
            if os.path.exists(DATA_FILE):
                try:
                    with open(DATA_FILE, "r", encoding="utf-8") as f: saved_data = json.load(f)
                except: pass

            session = get_session()
            any_changes = False

            # ê° ê²Œì‹œíŒ ìˆœíšŒ
            for board in TARGET_BOARDS:
                board_success = False
                
                # [new - ì¬ì‹œë„ ë¡œì§]
                for attempt in range(1, MAX_RETRIES + 1):
                    try:
                        if check_board(session, board, saved_data):
                            any_changes = True
                        board_success = True
                        break # ì„±ê³µ ì‹œ íƒˆì¶œ
                    except Exception as e:
                        print(f"âš  [{board['name']}] ì‹¤íŒ¨ ({attempt}/{MAX_RETRIES}): {e}")
                        if attempt < MAX_RETRIES:
                            time.sleep(RETRY_DELAY)
                
                # ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ê´€ë¦¬ì ì•Œë¦¼
                if not board_success:
                    send_simple_error_log(f"[{board['name']}] 3íšŒ ì ‘ì† ì‹¤íŒ¨", is_fatal=True)

            if any_changes:
                with open(DATA_FILE, "w", encoding="utf-8") as f:
                    json.dump(saved_data, f, ensure_ascii=False, indent=4)
                print("â˜‘ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            else:
                print("â˜’ ë³€ë™ ì‚¬í•­ ì—†ìŒ")

            print(f"ğŸ’¤ {CHECK_INTERVAL}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(CHECK_INTERVAL)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš  ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        send_simple_error_log(f"ë©”ì¸ ë£¨í”„ ì¢…ë£Œë¨\n{e}", is_fatal=True)

if __name__ == "__main__":
    run_bot()